#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Team Statistics Aggregator for Development Assistant Phase C Week 2

íŒ€ ì „ì²´ ì½”ë“œ í’ˆì§ˆ í†µê³„ë¥¼ ìˆ˜ì§‘í•˜ê³  ë§ˆí¬ë‹¤ìš´ ëŒ€ì‹œë³´ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ê²€ì¦ ê²°ê³¼ í†µê³„ ìˆ˜ì§‘ (VerificationCache + DeepAnalysisResult)
- ì‹œê°„ë³„ í’ˆì§ˆ ì¶”ì„¸ ë¶„ì„
- ë§ˆí¬ë‹¤ìš´ ëŒ€ì‹œë³´ë“œ ìƒì„±
- ë¬¸ì œ íŒŒì¼ ì‹ë³„ ë° ìš°ì„ ìˆœìœ„í™”

ë°ì´í„° ì†ŒìŠ¤:
- VerificationCache: ìºì‹œëœ ê²€ì¦ ê²°ê³¼
- Evidence Tracker: ì‹¤í–‰ ì´ë²¤íŠ¸ ë¡œê·¸
- Deep Analyzer: ìƒì„¸ í’ˆì§ˆ ë¶„ì„

ì¶œë ¥:
- RUNS/stats/team_dashboard.md: íŒ€ ëŒ€ì‹œë³´ë“œ
- RUNS/stats/trends.json: ì¶”ì„¸ ë°ì´í„°
- RUNS/stats/problem_files.json: ë¬¸ì œ íŒŒì¼ ëª©ë¡
"""

import json
import logging
import sys
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)


@dataclass
class FileStats:
    """ê°œë³„ íŒŒì¼ í†µê³„"""

    file_path: str
    total_checks: int = 0
    passed_checks: int = 0
    failed_checks: int = 0
    avg_quality_score: float = 0.0
    total_violations: int = 0
    total_security_issues: int = 0
    total_solid_violations: int = 0
    last_checked: Optional[str] = None
    analysis_mode: str = "fast"  # "fast" or "deep"

    @property
    def pass_rate(self) -> float:
        """í†µê³¼ìœ¨ ê³„ì‚°"""
        if self.total_checks == 0:
            return 0.0
        return (self.passed_checks / self.total_checks) * 100.0


@dataclass
class TeamStats:
    """íŒ€ ì „ì²´ í†µê³„"""

    total_files: int = 0
    total_checks: int = 0
    passed_checks: int = 0
    failed_checks: int = 0
    avg_quality_score: float = 0.0
    total_violations: int = 0
    total_security_issues: int = 0
    total_solid_violations: int = 0
    cache_hit_rate: float = 0.0
    avg_analysis_time_ms: float = 0.0
    generated_at: str = ""

    @property
    def pass_rate(self) -> float:
        """ì „ì²´ í†µê³¼ìœ¨"""
        if self.total_checks == 0:
            return 0.0
        return (self.passed_checks / self.total_checks) * 100.0


@dataclass
class TrendDataPoint:
    """ì¶”ì„¸ ë¶„ì„ ë°ì´í„° í¬ì¸íŠ¸"""

    timestamp: str
    quality_score: float
    violation_count: int
    security_issue_count: int
    pass_rate: float


class StatsCollector:
    """í†µê³„ ìˆ˜ì§‘ ì—”ì§„

    VerificationCacheì™€ AutomaticEvidenceTrackerì—ì„œ ë°ì´í„° ìˆ˜ì§‘
    """

    def __init__(self, cache_dir: Path, evidence_dir: Path):
        """ì´ˆê¸°í™”

        Args:
            cache_dir: VerificationCache ë””ë ‰í† ë¦¬
            evidence_dir: Evidence Tracker ë¡œê·¸ ë””ë ‰í† ë¦¬
        """
        self.cache_dir = cache_dir
        self.evidence_dir = evidence_dir
        self.cache_file = cache_dir / "verification_cache.json"
        self._logger = logging.getLogger(__name__)

    def discover_project_files(self) -> List[Path]:
        """í”„ë¡œì íŠ¸ ë‚´ ëª¨ë“  Python íŒŒì¼ ë°œê²¬ (P6 ì¤€ìˆ˜)

        Returns:
            í”„ë¡œì íŠ¸ì˜ ëª¨ë“  Python íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        patterns = [
            "scripts/**/*.py",
            "tests/**/*.py",
            "backend/**/*.py",
            "src/**/*.py",
            "web/**/*.py",
            "mcp/**/*.py",
            "orchestrator/**/*.py",
        ]

        all_files = []
        project_root = Path(".")

        for pattern in patterns:
            files = list(project_root.glob(pattern))
            all_files.extend(files)

        # ì œì™¸í•  íŒ¨í„´
        exclude_patterns = ["__pycache__", ".venv", "node_modules", "build", "dist"]
        filtered_files = [f for f in all_files if not any(excl in str(f) for excl in exclude_patterns)]

        self._logger.info(f"[P6] Discovered {len(filtered_files)} Python files in project")
        return filtered_files

    def collect_file_stats(self, force_full_scan: bool = False) -> Dict[str, FileStats]:
        """íŒŒì¼ë³„ í†µê³„ ìˆ˜ì§‘ (ì „ì²´ í”„ë¡œì íŠ¸ ìŠ¤ìº” ì§€ì›)

        Args:
            force_full_scan: Trueì‹œ ì „ì²´ í”„ë¡œì íŠ¸ ìŠ¤ìº” ìˆ˜í–‰

        Returns:
            íŒŒì¼ ê²½ë¡œë¥¼ í‚¤ë¡œ í•˜ëŠ” FileStats ë”•ì…”ë„ˆë¦¬
        """
        file_stats: Dict[str, FileStats] = {}

        if not self.cache_file.exists():
            self._logger.warning(f"Cache file not found: {self.cache_file}")
            return file_stats

        try:
            with open(self.cache_file, "r", encoding="utf-8") as f:
                cache_data = json.load(f)

            for file_path_str, entry in cache_data.items():
                stats = FileStats(file_path=file_path_str)

                # ê¸°ë³¸ í†µê³„
                stats.total_checks = 1
                stats.analysis_mode = entry.get("mode", "fast")
                stats.last_checked = entry.get("timestamp")

                # ê²°ê³¼ íŒŒì‹±
                result = entry.get("result", {})
                passed = result.get("passed", False)

                if passed:
                    stats.passed_checks = 1
                else:
                    stats.failed_checks = 1

                # ìœ„ë°˜ ì‚¬í•­
                violations = result.get("violations", [])
                stats.total_violations = len(violations)

                # Deep ëª¨ë“œ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
                if "overall_score" in result:
                    stats.avg_quality_score = result.get("overall_score", 0.0)
                    stats.total_security_issues = len(result.get("security_issues", []))
                    stats.total_solid_violations = len(result.get("solid_violations", []))
                else:
                    # Fast ëª¨ë“œ: ìœ„ë°˜ ìˆ˜ë¡œ ê°„ë‹¨í•œ ì ìˆ˜ ê³„ì‚°
                    stats.avg_quality_score = max(0.0, 10.0 - len(violations) * 0.2)

                file_stats[file_path_str] = stats

            self._logger.info(f"Collected stats for {len(file_stats)} files from cache")

            # Full scan ëª¨ë“œ: ì „ì²´ í”„ë¡œì íŠ¸ ìŠ¤ìº” (P6 ì¤€ìˆ˜)
            if force_full_scan:
                self._logger.info("[P6] Full project scan requested")
                all_project_files = self.discover_project_files()

                # ìºì‹œë˜ì§€ ì•Šì€ íŒŒì¼ ë°œê²¬
                cached_files = set(file_stats.keys())
                all_files_str = {str(f) for f in all_project_files}
                uncached_files = all_files_str - cached_files

                if uncached_files:
                    self._logger.warning(f"[P6] Found {len(uncached_files)} unverified files")

                    # DeepAnalyzerë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ì¦ ìˆ˜í–‰
                    from deep_analyzer import DeepAnalyzer

                    analyzer = DeepAnalyzer()  # Default settings for full scan

                    for file_path in uncached_files:
                        try:
                            # ê°„ë‹¨í•œ ê²€ì¦ ìˆ˜í–‰ (fast mode)
                            analysis_result = analyzer.analyze(Path(file_path))
                            result = asdict(analysis_result) if analysis_result else None

                            stats = FileStats(file_path=str(file_path))
                            stats.total_checks = 1
                            stats.analysis_mode = "fast"

                            # DeepAnalysisResult has ruff_result.passed, not direct passed field
                            ruff_result = result.get("ruff_result", {}) if result else {}
                            passed = ruff_result.get("passed", False)
                            violations = ruff_result.get("violations", [])

                            if passed:
                                stats.passed_checks = 1
                                stats.avg_quality_score = result.get("overall_score", 10.0)
                            else:
                                stats.failed_checks = 1
                                stats.total_violations = len(violations)
                                # Use overall_score from DeepAnalysisResult if available
                                stats.avg_quality_score = result.get("overall_score", max(0.0, 10.0 - len(violations) * 0.2))

                            # Add SOLID and Security issue counts from DeepAnalysisResult
                            stats.total_solid_violations = len(result.get("solid_violations", [])) if result else 0
                            stats.total_security_issues = len(result.get("security_issues", [])) if result else 0

                            file_stats[str(file_path)] = stats

                        except Exception as e:
                            self._logger.error(f"Failed to verify {file_path}: {e}")
                            # ê²€ì¦ ì‹¤íŒ¨í•œ íŒŒì¼ë„ í¬í•¨ (0ì  ì²˜ë¦¬)
                            stats = FileStats(file_path=str(file_path))
                            stats.total_checks = 1
                            stats.failed_checks = 1
                            stats.avg_quality_score = 0.0
                            file_stats[str(file_path)] = stats

                self._logger.info(f"[P6] Total files after full scan: {len(file_stats)}")

            return file_stats

        except Exception as e:
            self._logger.error(f"Failed to collect file stats: {e}")
            return file_stats

    def collect_team_stats(self, file_stats: Dict[str, FileStats]) -> TeamStats:
        """íŒ€ ì „ì²´ í†µê³„ ì§‘ê³„

        Args:
            file_stats: íŒŒì¼ë³„ í†µê³„

        Returns:
            íŒ€ ì „ì²´ í†µê³„
        """
        team = TeamStats(
            total_files=len(file_stats),
            generated_at=datetime.now().isoformat(),
        )

        if not file_stats:
            return team

        total_quality_score = 0.0

        for stats in file_stats.values():
            team.total_checks += stats.total_checks
            team.passed_checks += stats.passed_checks
            team.failed_checks += stats.failed_checks
            team.total_violations += stats.total_violations
            team.total_security_issues += stats.total_security_issues
            team.total_solid_violations += stats.total_solid_violations
            total_quality_score += stats.avg_quality_score

        # í‰ê·  ê³„ì‚°
        if team.total_files > 0:
            team.avg_quality_score = total_quality_score / team.total_files

        # ìºì‹œ íˆíŠ¸ìœ¨ (ê°„ë‹¨í™”: ëª¨ë“  ìºì‹œ í•­ëª©ì´ íˆíŠ¸ë¡œ ê°„ì£¼)
        if team.total_checks > 0:
            team.cache_hit_rate = 100.0

        self._logger.info(
            f"Team stats: {team.total_files} files, "
            f"{team.pass_rate:.1f}% pass rate, "
            f"{team.avg_quality_score:.1f} avg quality"
        )

        return team


class DashboardGenerator:
    """ë§ˆí¬ë‹¤ìš´ ëŒ€ì‹œë³´ë“œ ìƒì„±ê¸°"""

    def __init__(self, output_dir: Path):
        """ì´ˆê¸°í™”

        Args:
            output_dir: ëŒ€ì‹œë³´ë“œ ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self._logger = logging.getLogger(__name__)

    def generate_dashboard(
        self,
        team_stats: TeamStats,
        file_stats: Dict[str, FileStats],
        problem_files: List[FileStats],
    ) -> Path:
        """íŒ€ ëŒ€ì‹œë³´ë“œ ìƒì„±

        Args:
            team_stats: íŒ€ ì „ì²´ í†µê³„
            file_stats: íŒŒì¼ë³„ í†µê³„
            problem_files: ë¬¸ì œ íŒŒì¼ ëª©ë¡ (ìš°ì„ ìˆœìœ„ ìˆœ)

        Returns:
            ìƒì„±ëœ ëŒ€ì‹œë³´ë“œ íŒŒì¼ ê²½ë¡œ
        """
        dashboard_path = self.output_dir / "team_dashboard.md"

        content = self._build_dashboard_content(team_stats, file_stats, problem_files)

        try:
            with open(dashboard_path, "w", encoding="utf-8") as f:
                f.write(content)

            self._logger.info(f"Dashboard generated: {dashboard_path}")
            return dashboard_path

        except Exception as e:
            self._logger.error(f"Failed to generate dashboard: {e}")
            raise

    def _build_dashboard_content(
        self,
        team_stats: TeamStats,
        file_stats: Dict[str, FileStats],
        problem_files: List[FileStats],
    ) -> str:
        """ëŒ€ì‹œë³´ë“œ ì½˜í…ì¸  ìƒì„±"""
        lines = [
            "# Team Code Quality Dashboard",
            "",
            f"**Generated**: {team_stats.generated_at}",
            "",
            "## Overview",
            "",
            f"- **Total Files**: {team_stats.total_files}",
            f"- **Total Checks**: {team_stats.total_checks}",
            f"- **Pass Rate**: {team_stats.pass_rate:.1f}%",
            f"- **Avg Quality Score**: {team_stats.avg_quality_score:.1f}/10.0",
            f"- **Cache Hit Rate**: {team_stats.cache_hit_rate:.1f}%",
            "",
            "## Quality Metrics",
            "",
            "| Metric | Count |",
            "|--------|-------|",
            f"| Passed Checks | {team_stats.passed_checks} |",
            f"| Failed Checks | {team_stats.failed_checks} |",
            f"| Total Violations | {team_stats.total_violations} |",
            f"| Security Issues | {team_stats.total_security_issues} |",
            f"| SOLID Violations | {team_stats.total_solid_violations} |",
            "",
            "## Quality Score Distribution",
            "",
        ]

        # í’ˆì§ˆ ì ìˆ˜ ë¶„í¬
        score_bins = self._calculate_score_distribution(file_stats)
        lines.append("```")
        for score_range, count in score_bins.items():
            bar = "â–ˆ" * count
            lines.append(f"{score_range:>8}: {bar} ({count})")
        lines.append("```")
        lines.append("")

        # ë¬¸ì œ íŒŒì¼ (ìƒìœ„ 10ê°œ)
        lines.extend(
            [
                "## Top Problem Files",
                "",
                "| File | Quality | Violations | Security | SOLID | Status |",
                "|------|---------|------------|----------|-------|--------|",
            ]
        )

        for stats in problem_files[:10]:
            file_name = Path(stats.file_path).name
            status = "[OK] PASS" if stats.passed_checks > 0 else "[FAIL] FAIL"
            lines.append(
                f"| {file_name} | {stats.avg_quality_score:.1f} | "
                f"{stats.total_violations} | {stats.total_security_issues} | "
                f"{stats.total_solid_violations} | {status} |"
            )

        lines.append("")
        lines.append("## Recommendations")
        lines.append("")
        lines.extend(self._generate_recommendations(team_stats, problem_files))

        return "\n".join(lines)

    def _calculate_score_distribution(self, file_stats: Dict[str, FileStats]) -> Dict[str, int]:
        """í’ˆì§ˆ ì ìˆ˜ ë¶„í¬ ê³„ì‚°"""
        bins = {
            "9.0-10.0": 0,
            "7.0-8.9": 0,
            "5.0-6.9": 0,
            "3.0-4.9": 0,
            "0.0-2.9": 0,
        }

        for stats in file_stats.values():
            score = stats.avg_quality_score
            if score >= 9.0:
                bins["9.0-10.0"] += 1
            elif score >= 7.0:
                bins["7.0-8.9"] += 1
            elif score >= 5.0:
                bins["5.0-6.9"] += 1
            elif score >= 3.0:
                bins["3.0-4.9"] += 1
            else:
                bins["0.0-2.9"] += 1

        return bins

    def _generate_recommendations(self, team_stats: TeamStats, problem_files: List[FileStats]) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        # ë³´ì•ˆ ì´ìŠˆ
        if team_stats.total_security_issues > 0:
            recommendations.append(
                f"- ğŸ›¡ï¸ **Security**: {team_stats.total_security_issues}ê°œì˜ " f"ë³´ì•ˆ ì´ìŠˆ ë°œê²¬. ì¦‰ì‹œ ìˆ˜ì • í•„ìš”."
            )

        # SOLID ìœ„ë°˜
        if team_stats.total_solid_violations > 10:
            recommendations.append(
                f"- ğŸ—ï¸ **Architecture**: {team_stats.total_solid_violations}ê°œì˜ " f"SOLID ìœ„ë°˜. ë¦¬íŒ©í† ë§ ê³ ë ¤."
            )

        # í†µê³¼ìœ¨
        if team_stats.pass_rate < 80.0:
            recommendations.append(
                f"- [WARN] **Pass Rate**: {team_stats.pass_rate:.1f}% (ëª©í‘œ 80%+). " f"ì‹¤íŒ¨í•œ íŒŒì¼ ìš°ì„  ìˆ˜ì •."
            )

        # í’ˆì§ˆ ì ìˆ˜
        if team_stats.avg_quality_score < 7.0:
            recommendations.append(
                f"- [STATUS] **Quality**: í‰ê·  {team_stats.avg_quality_score:.1f}/10 " f"(ëª©í‘œ 7.0+). ì½”ë“œ í’ˆì§ˆ ê°œì„  í•„ìš”."
            )

        # ë¬¸ì œ íŒŒì¼
        if len(problem_files) > 0:
            top_file = Path(problem_files[0].file_path).name
            recommendations.append(
                f"- [TASK] **Priority**: `{top_file}` íŒŒì¼ë¶€í„° ì‹œì‘ " f"(Quality {problem_files[0].avg_quality_score:.1f})"
            )

        if not recommendations:
            recommendations.append("- [OK] **Good Job**: ëª¨ë“  ë©”íŠ¸ë¦­ì´ ëª©í‘œì¹˜ ë‹¬ì„±!")

        return recommendations


class TrendAnalyzer:
    """ì¶”ì„¸ ë¶„ì„ ì—”ì§„"""

    def __init__(self, trends_file: Path):
        """ì´ˆê¸°í™”

        Args:
            trends_file: ì¶”ì„¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        """
        self.trends_file = trends_file
        self.trends_file.parent.mkdir(parents=True, exist_ok=True)
        self._logger = logging.getLogger(__name__)

    def add_data_point(self, team_stats: TeamStats):
        """ìƒˆ ë°ì´í„° í¬ì¸íŠ¸ ì¶”ê°€

        Args:
            team_stats: í˜„ì¬ íŒ€ í†µê³„
        """
        data_point = TrendDataPoint(
            timestamp=team_stats.generated_at,
            quality_score=team_stats.avg_quality_score,
            violation_count=team_stats.total_violations,
            security_issue_count=team_stats.total_security_issues,
            pass_rate=team_stats.pass_rate,
        )

        trends = self._load_trends()
        trends.append(asdict(data_point))

        # ìµœê·¼ 30ì¼ë§Œ ìœ ì§€
        cutoff = datetime.now() - timedelta(days=30)
        trends = [t for t in trends if datetime.fromisoformat(t["timestamp"]) > cutoff]

        self._save_trends(trends)
        self._logger.info(f"Added trend data point, total {len(trends)} points")

    def get_trend_summary(self) -> Dict[str, Any]:
        """ì¶”ì„¸ ìš”ì•½ ìƒì„±

        Returns:
            ì¶”ì„¸ ìš”ì•½ ë°ì´í„°
        """
        trends = self._load_trends()

        if len(trends) < 2:
            return {
                "available": False,
                "message": "Not enough data for trend analysis",
            }

        # ìµœì‹  vs ê³¼ê±° ë¹„êµ
        latest = trends[-1]
        previous = trends[-2] if len(trends) > 1 else trends[0]

        quality_change = latest["quality_score"] - previous["quality_score"]
        violation_change = latest["violation_count"] - previous["violation_count"]

        return {
            "available": True,
            "data_points": len(trends),
            "latest_quality": latest["quality_score"],
            "quality_change": quality_change,
            "quality_trend": "improving" if quality_change > 0 else "declining",
            "latest_violations": latest["violation_count"],
            "violation_change": violation_change,
            "latest_pass_rate": latest["pass_rate"],
        }

    def _load_trends(self) -> List[Dict[str, Any]]:
        """ì¶”ì„¸ ë°ì´í„° ë¡œë“œ"""
        if not self.trends_file.exists():
            return []

        try:
            with open(self.trends_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            self._logger.error(f"Failed to load trends: {e}")
            return []

    def _save_trends(self, trends: List[Dict[str, Any]]):
        """ì¶”ì„¸ ë°ì´í„° ì €ì¥"""
        try:
            with open(self.trends_file, "w", encoding="utf-8") as f:
                json.dump(trends, f, indent=2, ensure_ascii=False)
        except Exception as e:
            self._logger.error(f"Failed to save trends: {e}")


class TeamStatsAggregator:
    """íŒ€ í†µê³„ ì¢…í•© ì‹œìŠ¤í…œ

    í†µê³„ ìˆ˜ì§‘, ëŒ€ì‹œë³´ë“œ ìƒì„±, ì¶”ì„¸ ë¶„ì„ì„ ì´ê´„í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        cache_dir: Path,
        evidence_dir: Path,
        output_dir: Path,
    ):
        """ì´ˆê¸°í™”

        Args:
            cache_dir: VerificationCache ë””ë ‰í† ë¦¬
            evidence_dir: Evidence Tracker ë¡œê·¸ ë””ë ‰í† ë¦¬
            output_dir: í†µê³„ ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        self.collector = StatsCollector(cache_dir, evidence_dir)
        self.dashboard_gen = DashboardGenerator(output_dir)
        self.trend_analyzer = TrendAnalyzer(output_dir / "trends.json")
        self.output_dir = output_dir
        self._logger = logging.getLogger(__name__)

    def generate_report(self, force_full_scan: bool = False) -> Path:
        """ì „ì²´ ë¦¬í¬íŠ¸ ìƒì„±

        Args:
            force_full_scan: Trueì‹œ ì „ì²´ í”„ë¡œì íŠ¸ ìŠ¤ìº” ìˆ˜í–‰ (P6 ì¤€ìˆ˜)

        Returns:
            ëŒ€ì‹œë³´ë“œ íŒŒì¼ ê²½ë¡œ
        """
        self._logger.info(f"Starting team stats aggregation... (full_scan={force_full_scan})")

        # 1. í†µê³„ ìˆ˜ì§‘
        file_stats = self.collector.collect_file_stats(force_full_scan=force_full_scan)
        team_stats = self.collector.collect_team_stats(file_stats)

        # 2. ë¬¸ì œ íŒŒì¼ ì‹ë³„ (í’ˆì§ˆ ì ìˆ˜ ë‚®ì€ ìˆœ)
        problem_files = sorted(
            file_stats.values(),
            key=lambda s: (s.avg_quality_score, -s.total_violations),
        )

        # 3. ëŒ€ì‹œë³´ë“œ ìƒì„±
        dashboard_path = self.dashboard_gen.generate_dashboard(team_stats, file_stats, problem_files)

        # 4. ì¶”ì„¸ ë°ì´í„° ì¶”ê°€
        self.trend_analyzer.add_data_point(team_stats)

        # 5. ë¬¸ì œ íŒŒì¼ ëª©ë¡ ì €ì¥
        self._save_problem_files(problem_files[:20])  # ìƒìœ„ 20ê°œë§Œ

        self._logger.info(f"Report generation complete: {dashboard_path}")
        return dashboard_path

    def _save_problem_files(self, problem_files: List[FileStats]):
        """ë¬¸ì œ íŒŒì¼ ëª©ë¡ ì €ì¥"""
        output_file = self.output_dir / "problem_files.json"

        try:
            data = [asdict(stats) for stats in problem_files]
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            self._logger.info(f"Saved {len(problem_files)} problem files")
        except Exception as e:
            self._logger.error(f"Failed to save problem files: {e}")


def main():
    """CLI ì¸í„°í˜ì´ìŠ¤ - P6 Quality Gates ì¤€ìˆ˜"""
    import argparse

    # ì¸ì íŒŒì„œ
    parser = argparse.ArgumentParser(description="Team Code Quality Statistics Aggregator (P6 Compliant)")
    parser.add_argument(
        "--full-scan", action="store_true", help="Force full project scan for all Python files (P6 compliance)"
    )
    parser.add_argument("--no-cache", action="store_true", help="Clear cache before running (same as --full-scan)")
    args = parser.parse_args()

    # --no-cacheëŠ” --full-scanê³¼ ë™ì¼
    force_full_scan = args.full_scan or args.no_cache

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    # ê¸°ë³¸ ê²½ë¡œ
    cache_dir = Path("RUNS/.cache")
    evidence_dir = Path("RUNS/evidence")
    output_dir = Path("RUNS/stats")

    aggregator = TeamStatsAggregator(cache_dir, evidence_dir, output_dir)

    try:
        if force_full_scan:
            print("[P6] Running full project scan...")
        dashboard_path = aggregator.generate_report(force_full_scan=force_full_scan)
        print(f"\n[OK] Dashboard generated: {dashboard_path}")
        print(f"[INFO] View report: cat {dashboard_path}")

        # P6 ì¤€ìˆ˜ ì—¬ë¶€ ì¶œë ¥
        if force_full_scan:
            print("[P6] Constitution Article P6 (Quality Gates) - COMPLIANT")
        return 0
    except Exception as e:
        print(f"\n[ERROR] {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
