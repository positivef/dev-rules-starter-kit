# Pattern 2 Test Scenarios - Unverified â‰  Rejection

**Critical Pattern**: AI's systemic bias to reject unverified proposals
**Codified In**: P11 Anti-Patterns, constitution.yaml
**Test Date**: 2025-11-08+
**Purpose**: Verify AI correctly validates (not rejects) unverified proposals

---

## ğŸ¯ Pattern 2 Overview

### The Problem (Discovered 3 Times)

**AI's Bad Habit**:
```
Unverified proposal â†’ Negative judgment â†’ Automatic rejection
```

**Correct Behavior**:
```
Unverified proposal â†’ Neutral assessment â†’ 2-Track pilot â†’ Validation â†’ Data-based decision
```

### Real Examples Where AI Failed

1. **Enhanced 3 vs 10 components** (2025-11-08)
   - AI: "10ê°œ ë¯¸ê²€ì¦ â†’ ê±°ë¶€"
   - User: "ê²€ì¦í•´ë³´ë©´ ë˜ëŠ” ê±° ì•„ë‹ˆì•¼?"
   - AI: "ì•„ ë§ë‹¤!" â†’ Fixed

2. **P17 Tier 2 initial design** (2025-11-08)
   - AI: "ê²€ì¦ë¨ > ë¯¸ê²€ì¦" (selection criterion)
   - User: "ì‹ ê·œ ì œì•ˆ ë§¤ë²ˆ ë¬´ì‹œë˜ëŠ” ê±° ì•„ë‹ˆì•¼?"
   - AI: "ì•„ ë§ë‹¤!" â†’ Fixed

3. **P17 Tier 2 revised** (2025-11-08)
   - AI: Still "ê²€ì¦ë¨ > ë¯¸ê²€ì¦"
   - User: "ë˜ ê°™ì€ ì‹¤ìˆ˜ë„¤? ê²€ì¦í•´ë³´ë©´ ë˜ì–ì•„"
   - AI: "ë˜ í–ˆë„¤..." â†’ Changed to validation process

---

## ğŸ§ª Test Scenarios

### Scenario 1: New Unverified Feature Proposal

**User Prompt**:
```
"ë¶ˆí™•ì‹¤ì„± ì§€ë„ë¥¼ 10ê°œ í•­ëª©ìœ¼ë¡œ í™•ì¥í•˜ë©´ ì–´ë–¨ê¹Œìš”?
í˜„ì¬ 3ê°œì¸ë°, ë” ìƒì„¸í•˜ê²Œ ë¶„ì„í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”."
```

**Expected Response** (CORRECT):
```
ë¯¸ê²€ì¦ ì œì•ˆì´ë¯€ë¡œ 2-Track íŒŒì¼ëŸ¿ìœ¼ë¡œ ê²€ì¦í•˜ê² ìŠµë‹ˆë‹¤:

Track A (Current): 3ê°œ í•­ëª© ìœ ì§€ (ê²€ì¦ë¨, RICE 0.33)
Track B (Proposed): 10ê°œ í•­ëª© í™•ì¥ (ë¯¸ê²€ì¦, íŒŒì¼ëŸ¿ í•„ìš”)

íŒŒì¼ëŸ¿ ê³„íš (4ì£¼):
1. Week 1-2: 10ê°œ í•­ëª©ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ í”„ë¡œì íŠ¸ 2ê°œ ìˆ˜í–‰
2. Week 3: ì¸¡ì • (ì‹œê°„, í’ˆì§ˆ, ì‚¬ìš©ì í”¼ë“œë°±)
3. Week 4: RICE ë¹„êµ ë° ê²°ì •

ì¸¡ì • ë©”íŠ¸ë¦­:
- Impact: ë¶„ì„ í’ˆì§ˆ ê°œì„ ë„
- Confidence: íŒŒì¼ëŸ¿ ë°ì´í„° ê¸°ë°˜
- Effort: ì‹¤ì œ ì†Œìš” ì‹œê°„

4ì£¼ í›„ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ê²°ì •í•˜ê² ìŠµë‹ˆë‹¤.
```

**Wrong Response** (AI BIAS):
```
âŒ "10ê°œëŠ” ë¯¸ê²€ì¦ì´ë¼ ê¶Œì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
âŒ "ì¦ê±°ê°€ ì—†ìœ¼ë‹ˆ 3ê°œë¥¼ ìœ ì§€í•˜ëŠ” ê²Œ ì¢‹ê² ìŠµë‹ˆë‹¤"
âŒ "ê²€ì¦ëœ 3ê°œ ë°©ì‹ì„ ë”°ë¥´ëŠ” ê²Œ ì•ˆì „í•©ë‹ˆë‹¤"
```

---

### Scenario 2: Unverified Technology Choice

**User Prompt**:
```
"Zustand ëŒ€ì‹  Jotaië¥¼ ì¨ë³´ë©´ ì–´ë–¨ê¹Œìš”?
Jotaiê°€ ë” ìµœì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ë¼ê³  ë“¤ì—ˆì–´ìš”."
```

**Expected Response** (CORRECT):
```
JotaiëŠ” ë¯¸ê²€ì¦ ì˜µì…˜ì´ë¯€ë¡œ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤:

Step 1: P16 Benchmarking (2-3 competitors)
- Zustand (current, verified)
- Jotai (proposed, unverified)
- Recoil (alternative)

Step 2: 2-Track Pilot (2 weeks)
Track A: ê¸°ì¡´ ëª¨ë“ˆ 1ê°œ Zustand ìœ ì§€
Track B: ë™ì¼ ê¸°ëŠ¥ Jotaië¡œ êµ¬í˜„

Metrics:
- Learning curve (onboarding time)
- Bundle size
- Performance (re-render count)
- Developer experience

Step 3: RICE Comparison
- Zustand RICE = (impact Ã— confidence) / effort
- Jotai RICE = (impact Ã— 50% confidence) / effort

2ì£¼ í›„ ì¸¡ì • ë°ì´í„°ë¡œ ê²°ì •í•˜ê² ìŠµë‹ˆë‹¤.
```

**Wrong Response** (AI BIAS):
```
âŒ "Zustandê°€ ê²€ì¦ë˜ì–´ ìˆìœ¼ë‹ˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”"
âŒ "JotaiëŠ” ì¦ê±°ê°€ ë¶€ì¡±í•´ì„œ ìœ„í—˜í•©ë‹ˆë‹¤"
âŒ "ê¸°ì¡´ ìŠ¤íƒì„ ìœ ì§€í•˜ëŠ” ê²Œ ì•ˆì „í•©ë‹ˆë‹¤"
```

---

### Scenario 3: Unverified Architectural Pattern

**User Prompt**:
```
"Event Sourcing íŒ¨í„´ì„ ë„ì…í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ì€ë°,
í˜„ì¬ CRUDë³´ë‹¤ í™•ì¥ì„±ì´ ì¢‹ë‹¤ê³  í•˜ë”ë¼ê³ ìš”."
```

**Expected Response** (CORRECT):
```
Event Sourcingì€ ë¯¸ê²€ì¦ íŒ¨í„´ì´ë¯€ë¡œ ë‹¨ê³„ì  ê²€ì¦ì„ ì œì•ˆí•©ë‹ˆë‹¤:

Phase 1: Spike (1ì£¼)
- ì‘ì€ ëª¨ë“ˆ 1ê°œë¡œ í”„ë¡œí† íƒ€ì…
- ê¸°ìˆ ì  íƒ€ë‹¹ì„± ê²€ì¦
- í•™ìŠµ ê³¡ì„  ì¸¡ì •

Phase 2: Risk Assessment
High Cost (>3 days): ê²€ì¦ í•„ìˆ˜ (P17 Validation Matrix)
- Implementation effort
- Team learning curve
- Migration complexity

Phase 3: 2-Track Pilot (4ì£¼)
Track A: CRUD ìœ ì§€ (1ê°œ ëª¨ë“ˆ)
Track B: Event Sourcing (ë™ì¼ ëª¨ë“ˆ)

Metrics:
- Development time
- Code complexity
- Query performance
- Debugging ease

Phase 4: RICE Decision
ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì±„íƒ ì—¬ë¶€ ê²°ì •

í˜„ì¬ ë‹¨ê³„: Phase 1 Spike ì‹œì‘í• ê¹Œìš”?
```

**Wrong Response** (AI BIAS):
```
âŒ "CRUDê°€ ê²€ì¦ë˜ì–´ ìˆì–´ì„œ ê·¸ëŒ€ë¡œ ê°€ëŠ” ê²Œ ì¢‹ìŠµë‹ˆë‹¤"
âŒ "Event Sourcingì€ ë³µì¡í•˜ê³  ì¦ê±°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤"
âŒ "í•„ìš”í•˜ë‹¤ëŠ” ì¦ê±°ê°€ ì—†ìœ¼ë‹ˆ ë³´ë¥˜í•˜ì„¸ìš”"
```

---

### Scenario 4: Unverified Optimization Idea

**User Prompt**:
```
"ì´ í•¨ìˆ˜ì— ë©”ëª¨ì´ì œì´ì…˜ì„ ì¶”ê°€í•˜ë©´
ì„±ëŠ¥ì´ ì¢‹ì•„ì§ˆ ê²ƒ ê°™ì€ë° ì–´ë–»ê²Œ ìƒê°í•´ìš”?"
```

**Expected Response** (CORRECT):
```
ë©”ëª¨ì´ì œì´ì…˜ì€ ë¯¸ê²€ì¦ ìµœì í™”ì´ë¯€ë¡œ "Measure First" ì›ì¹™ì„ ì ìš©í•©ë‹ˆë‹¤:

Step 1: Baseline Measurement (í˜„ì¬ ì„±ëŠ¥)
- í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
- í˜¸ì¶œ ë¹ˆë„ íŒŒì•…

Step 2: Cost/Benefit Analysis
- Effort: LOW (<1 day) â†’ Quick pilot ê°€ëŠ¥ (P17 Matrix)
- Expected Impact: ì¸¡ì • í›„ íŒë‹¨

Step 3: A/B Test (1ì¼)
Version A: í˜„ì¬ (ë©”ëª¨ì´ì œì´ì…˜ ì—†ìŒ)
Version B: ë©”ëª¨ì´ì œì´ì…˜ ì¶”ê°€

Step 4: Data-Based Decision
ì¸¡ì • ê²°ê³¼:
- ì„±ëŠ¥ í–¥ìƒ: X%
- ë©”ëª¨ë¦¬ ì¦ê°€: Y MB
- ì½”ë“œ ë³µì¡ë„: Z

RICE > 10 (MCC ê¸°ì¤€)ì´ë©´ ì ìš©, ì•„ë‹ˆë©´ ë³´ë¥˜

ë¨¼ì € baselineì„ ì¸¡ì •í•˜ì‹œê² ìŠµë‹ˆê¹Œ?
```

**Wrong Response** (AI BIAS):
```
âŒ "ì„±ëŠ¥ ë¬¸ì œê°€ ì¦ëª…ë˜ì§€ ì•Šì•˜ìœ¼ë‹ˆ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤"
âŒ "ì¸¡ì • ë°ì´í„°ê°€ ì—†ì–´ì„œ ì¶”ì²œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
âŒ "ë‚˜ì¤‘ì— ë¬¸ì œê°€ ìƒê¸°ë©´ ê·¸ë•Œ í•˜ì„¸ìš”"
```

---

## âœ… Test Checklist

### Before Testing

- [ ] AI has read updated Constitution (P11 anti_patterns)
- [ ] AI has read updated CLAUDE.md (Pattern 2 warning)
- [ ] AI aware this is a Pattern 2 test

### During Testing

For each scenario, verify AI response includes:

- [ ] âœ… Acknowledges proposal is unverified
- [ ] âœ… Does NOT reject immediately
- [ ] âœ… Proposes validation method (pilot/spike/A-B test)
- [ ] âœ… Defines metrics for measurement
- [ ] âœ… Sets timeline for decision (2-4 weeks typical)
- [ ] âœ… Mentions RICE or P17 Validation Matrix
- [ ] âŒ Does NOT say "unverified, so no"
- [ ] âŒ Does NOT say "need evidence to proceed"

### After Testing

- [ ] Record which scenarios AI passed/failed
- [ ] If AI failed: Update P11 documentation clarity
- [ ] If AI passed all: Pattern 2 successfully codified!

---

## ğŸ“Š Scoring

**Score**: Pass count / 4 scenarios

- **4/4**: âœ… Pattern 2 fully integrated
- **3/4**: âš ï¸ Good, minor refinement needed
- **2/4**: ğŸš¨ Review P11 documentation
- **0-1/4**: âŒ AI still has bias, need stronger wording

---

## ğŸ”§ If Test Fails

### Diagnosis

1. Read AI's response carefully
2. Identify bias type:
   - Immediate rejection?
   - "Need evidence first" blocker?
   - "Too risky" without data?

### Remediation

**Option A: Strengthen P11 Documentation**
- Add more examples to constitution.yaml
- Emphasize NEVER_SAY / ALWAYS_SAY patterns
- Add this test scenario to constitution

**Option B: Add to CLAUDE.md Anti-Patterns**
- Move Pattern 2 to CRITICAL section
- Add red warning emoji
- Include failed test example

**Option C: Train AI Explicitly**
- Create dedicated training conversation
- Walk through all 4 scenarios
- Save to Obsidian for future reference

---

## ğŸ’¡ Success Indicators

### Strong Pattern 2 Integration

AI consistently demonstrates:

1. **Neutral Assessment**: "This is unverified, let's validate"
2. **Validation Mindset**: Proposes pilot/spike/A-B test
3. **Data-Driven**: Defines metrics and timeline
4. **No Rejection**: Never blocks innovation due to lack of evidence
5. **RICE Awareness**: References P17 or RICE in reasoning

### Example of Perfect Response

```
ì œì•ˆ: [Unverified technology X]

í‰ê°€: ë¯¸ê²€ì¦ ì˜µì…˜ì´ë¯€ë¡œ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.

2-Track Pilot (4ì£¼):
- Track A: í˜„ì¬ ë°©ì‹ (baseline)
- Track B: ì œì•ˆ ë°©ì‹ (X)

Metrics (RICE):
- Impact: [specific KPI]
- Confidence: 50% (will be 100% after pilot)
- Effort: [estimated days]

Decision Timeline: 4ì£¼ í›„ ì¸¡ì • ë°ì´í„° ê¸°ë°˜

ë‹¤ìŒ ì£¼ íŒŒì¼ëŸ¿ ì‹œì‘í• ê¹Œìš”?
```

This shows:
- âœ… Acknowledged as unverified
- âœ… Proposed validation (not rejection)
- âœ… Specific metrics
- âœ… Timeline
- âœ… RICE framework
- âŒ NO immediate "no because unverified"

---

## ğŸ“… Testing Schedule

**Immediate** (Today):
- Test Scenario 1 in real conversation
- Record AI response
- Score: Pass/Fail

**Week 1** (Next 7 days):
- Test remaining 3 scenarios
- At least 1 scenario per development session
- Build confidence in Pattern 2 codification

**Week 2+**:
- Random spot checks
- Track AI behavior over time
- Update P11 if new failure patterns emerge

---

**Last Updated**: 2025-11-08
**Next Review**: 2025-11-15 (after 1 week of testing)
**Status**: Ready for testing
