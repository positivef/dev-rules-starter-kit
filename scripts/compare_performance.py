#!/usr/bin/env python3
"""
Compare OLD (simple) vs NEW (hybrid) error resolution system performance

Usage:
    python scripts/compare_performance.py

Requires:
    - RUNS/benchmark/simple_latest.json
    - RUNS/benchmark/hybrid_latest.json

Generated by running:
    python scripts/benchmark_error_resolution.py --mode simple
    python scripts/benchmark_error_resolution.py --mode hybrid
"""

import json
import sys
from pathlib import Path
from typing import Dict


class PerformanceComparison:
    """Compare performance between OLD and NEW systems"""

    def __init__(self):
        self.benchmark_dir = Path("RUNS/benchmark")
        self.simple_path = self.benchmark_dir / "simple_latest.json"
        self.hybrid_path = self.benchmark_dir / "hybrid_latest.json"

    def load_results(self) -> tuple[Dict, Dict]:
        """Load benchmark results"""
        if not self.simple_path.exists():
            print(f"[ERROR] Simple benchmark not found: {self.simple_path}")
            print("Run: python scripts/benchmark_error_resolution.py --mode simple")
            sys.exit(1)

        if not self.hybrid_path.exists():
            print(f"[ERROR] Hybrid benchmark not found: {self.hybrid_path}")
            print("Run: python scripts/benchmark_error_resolution.py --mode hybrid")
            sys.exit(1)

        with open(self.simple_path, "r", encoding="utf-8") as f:
            simple = json.load(f)

        with open(self.hybrid_path, "r", encoding="utf-8") as f:
            hybrid = json.load(f)

        return simple, hybrid

    def compare(self):
        """Compare and print results"""
        simple, hybrid = self.load_results()

        print("=" * 80)
        print("PERFORMANCE COMPARISON: OLD (Simple) vs NEW (Hybrid)")
        print("=" * 80)

        print("\n[TEST INFO]")
        print(f"  OLD System - Timestamp: {simple['timestamp']}")
        print(f"  NEW System - Timestamp: {hybrid['timestamp']}")
        print(f"  Iterations: {simple['iterations']}")

        self._print_automation_comparison(simple, hybrid)
        self._print_time_comparison(simple, hybrid)
        self._print_tier_comparison(simple, hybrid)
        self._print_user_intervention_comparison(simple, hybrid)
        self._print_accuracy_comparison(simple, hybrid)
        self._print_roi_calculation(simple, hybrid)
        self._print_recommendations(simple, hybrid)

        # Generate summary report
        self._save_comparison_report(simple, hybrid)

    def _print_automation_comparison(self, simple: Dict, hybrid: Dict):
        """Compare automation rates"""
        old_rate = simple["automation_rate"]
        new_rate = hybrid["automation_rate"]
        improvement = ((new_rate - old_rate) / old_rate * 100) if old_rate > 0 else float("inf")

        print("\n[AUTOMATION RATE]")
        print(f"  OLD: {old_rate:.1%}")
        print(f"  NEW: {new_rate:.1%}")
        print(f"  Improvement: +{improvement:.1f}%")

        if improvement > 300:
            print("  Status: [EXCELLENT] Major improvement achieved!")
        elif improvement > 200:
            print("  Status: [GOOD] Significant improvement")
        elif improvement > 100:
            print("  Status: [MODERATE] Noticeable improvement")
        else:
            print("  Status: [NEEDS REVIEW] Lower than expected")

    def _print_time_comparison(self, simple: Dict, hybrid: Dict):
        """Compare resolution times"""
        old_time = simple["avg_resolution_time_ms"]
        new_time = hybrid["avg_resolution_time_ms"]
        reduction = ((old_time - new_time) / old_time * 100) if old_time > 0 else 0

        print("\n[RESOLUTION TIME]")
        print(f"  OLD: {old_time:.1f}ms ({old_time/1000:.2f}s)")
        print(f"  NEW: {new_time:.1f}ms ({new_time/1000:.2f}s)")
        print(f"  Reduction: -{reduction:.1f}%")
        print(f"  Time Saved: {old_time - new_time:.1f}ms per error")

        if reduction > 80:
            print("  Status: [EXCELLENT] Massive speedup!")
        elif reduction > 50:
            print("  Status: [GOOD] Significant speedup")
        elif reduction > 20:
            print("  Status: [MODERATE] Noticeable speedup")
        else:
            print("  Status: [NEEDS REVIEW] Minimal speedup")

    def _print_tier_comparison(self, simple: Dict, hybrid: Dict):
        """Compare tier distribution"""
        print("\n[TIER DISTRIBUTION]")

        print("\n  Tier 1 (Obsidian - Local)")
        print(f"    OLD: {simple['tier1_hits']} ({simple['tier1_hits']/simple['total_errors']*100:.1f}%)")
        print(f"    NEW: {hybrid['tier1_hits']} ({hybrid['tier1_hits']/hybrid['total_errors']*100:.1f}%)")
        improvement = hybrid["tier1_hits"] - simple["tier1_hits"]
        print(f"    Change: +{improvement} ({improvement/simple['total_errors']*100:+.1f}%p)")

        print("\n  Tier 2 (Context7 - Official Docs)")
        simple_t2 = simple["tier2_auto"] + simple.get("tier2_confirmed", 0)
        hybrid_t2 = hybrid["tier2_auto"] + hybrid.get("tier2_confirmed", 0)
        print(f"    OLD: {simple_t2} ({simple_t2/simple['total_errors']*100:.1f}%)")
        print(f"    NEW: {hybrid_t2} ({hybrid_t2/hybrid['total_errors']*100:.1f}%)")
        print(f"      - Auto-applied: {hybrid['tier2_auto']} (NEW feature!)")
        print(f"      - User confirmed: {hybrid.get('tier2_confirmed', 0)}")

        print("\n  Tier 3 (User Intervention)")
        print(f"    OLD: {simple['tier3_hits']} ({simple['tier3_hits']/simple['total_errors']*100:.1f}%)")
        print(f"    NEW: {hybrid['tier3_hits']} ({hybrid['tier3_hits']/hybrid['total_errors']*100:.1f}%)")
        reduction = simple["tier3_hits"] - hybrid["tier3_hits"]
        print(f"    Reduction: -{reduction} ({reduction/simple['total_errors']*100:.1f}%p)")

    def _print_user_intervention_comparison(self, simple: Dict, hybrid: Dict):
        """Compare user intervention rates"""
        old_rate = simple["user_intervention_rate"]
        new_rate = hybrid["user_intervention_rate"]
        reduction_pp = (old_rate - new_rate) * 100

        print("\n[USER INTERVENTION]")
        print(f"  OLD: {old_rate:.1%}")
        print(f"  NEW: {new_rate:.1%}")
        print(f"  Reduction: -{reduction_pp:.1f} percentage points")

        # Calculate time saved
        avg_user_time_minutes = 5  # Average user response time
        errors_per_day = 5
        old_interventions = old_rate * errors_per_day
        new_interventions = new_rate * errors_per_day
        daily_time_saved = (old_interventions - new_interventions) * avg_user_time_minutes

        print(f"\n  Time Saved per Day: {daily_time_saved:.1f} minutes")
        print(f"  Time Saved per Week: {daily_time_saved * 5:.1f} minutes")
        print(f"  Time Saved per Month: {daily_time_saved * 20:.1f} minutes ({daily_time_saved * 20 / 60:.1f} hours)")

    def _print_accuracy_comparison(self, simple: Dict, hybrid: Dict):
        """Compare accuracy (only for hybrid)"""
        print("\n[ACCURACY]")
        print("  OLD: N/A (no auto-apply)")

        if hybrid.get("accuracy") is not None:
            accuracy = hybrid["accuracy"]
            print(f"  NEW: {accuracy:.1%}")

            if accuracy >= 0.95:
                print("  Status: [EXCELLENT] High accuracy, safe to use")
            elif accuracy >= 0.90:
                print("  Status: [GOOD] Acceptable accuracy")
            elif accuracy >= 0.85:
                print("  Status: [MODERATE] Monitor closely")
            else:
                print("  Status: [WARNING] Review auto-apply decisions")
        else:
            print("  NEW: Not measured")

    def _print_roi_calculation(self, simple: Dict, hybrid: Dict):
        """Calculate return on investment"""
        print("\n[ROI CALCULATION]")

        # Setup cost
        setup_hours = 40
        print(f"  Setup Cost: {setup_hours} hours")

        # Time savings
        errors_per_day = 5
        old_time_per_error_minutes = 5
        new_time_per_error_minutes = 0.5  # 30 seconds

        old_interventions = simple["user_intervention_rate"] * errors_per_day
        new_interventions = hybrid["user_intervention_rate"] * errors_per_day

        old_daily_time = old_interventions * old_time_per_error_minutes
        new_daily_time = new_interventions * old_time_per_error_minutes

        # Account for auto-applied errors (much faster)
        auto_count = hybrid["automation_rate"] * errors_per_day
        new_daily_time += auto_count * new_time_per_error_minutes

        daily_savings = old_daily_time - new_daily_time
        weekly_savings = daily_savings * 5
        monthly_savings = daily_savings * 20
        yearly_savings = daily_savings * 240  # 240 work days

        print("\n  Time Savings:")
        print(f"    Per Day: {daily_savings:.1f} minutes")
        print(f"    Per Week: {weekly_savings:.1f} minutes ({weekly_savings/60:.1f} hours)")
        print(f"    Per Month: {monthly_savings:.1f} minutes ({monthly_savings/60:.1f} hours)")
        print(f"    Per Year: {yearly_savings:.1f} minutes ({yearly_savings/60:.1f} hours)")

        # Break-even
        break_even_months = (setup_hours * 60) / monthly_savings if monthly_savings > 0 else float("inf")

        print(f"\n  Break-even: {break_even_months:.1f} months")

        # ROI
        roi_1_year = ((yearly_savings / 60 - setup_hours) / setup_hours * 100) if setup_hours > 0 else 0
        roi_3_year = ((yearly_savings / 60 * 3 - setup_hours) / setup_hours * 100) if setup_hours > 0 else 0

        print("\n  ROI:")
        print(f"    1 Year: {roi_1_year:+.1f}%")
        print(f"    3 Years: {roi_3_year:+.1f}%")

        if roi_1_year > 100:
            print("  Status: [EXCELLENT] Strong positive ROI")
        elif roi_1_year > 50:
            print("  Status: [GOOD] Positive ROI")
        elif roi_1_year > 0:
            print("  Status: [MODERATE] Modest ROI")
        else:
            print("  Status: [REVIEW] Negative ROI in first year")

    def _print_recommendations(self, simple: Dict, hybrid: Dict):
        """Print recommendations based on results"""
        print("\n[RECOMMENDATIONS]")

        automation_rate = hybrid["automation_rate"]
        accuracy = hybrid.get("accuracy", 1.0)

        if automation_rate >= 0.70 and accuracy >= 0.95:
            print("  [EXCELLENT] System performing optimally")
            print("  - Consider progressive enhancement (lower threshold to 92%)")
            print("  - Monitor for 2 weeks before next adjustment")
        elif automation_rate >= 0.50 and accuracy >= 0.90:
            print("  [GOOD] System performing well")
            print("  - Continue monitoring")
            print("  - Add more patterns to whitelist")
        elif automation_rate >= 0.30:
            print("  [MODERATE] Room for improvement")
            print("  - Review MEDIUM confidence cases")
            print("  - Consider adding common patterns to whitelist")
        else:
            print("  [NEEDS ATTENTION] Lower than expected automation")
            print("  - Review configuration")
            print("  - Check Context7 connectivity")
            print("  - Verify Obsidian sync is working")

        # Circuit breaker check
        print("\n  Circuit Breaker:")
        if accuracy is not None and accuracy < 0.90:
            print("    [WARNING] Accuracy below 90%, circuit breaker may trigger")
            print("    - Review auto-applied decisions")
            print("    - Add problematic patterns to blacklist")
        else:
            print("    [OK] No issues detected")

    def _save_comparison_report(self, simple: Dict, hybrid: Dict):
        """Save comparison report to file"""
        report_dir = Path("RUNS/reports")
        report_dir.mkdir(parents=True, exist_ok=True)

        from datetime import datetime

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = report_dir / f"comparison_{timestamp}.md"

        # Calculate key metrics
        auto_improvement = (
            ((hybrid["automation_rate"] - simple["automation_rate"]) / simple["automation_rate"] * 100)
            if simple["automation_rate"] > 0
            else 0
        )

        old_time = simple["avg_resolution_time_ms"]
        new_time = hybrid["avg_resolution_time_ms"]
        time_reduction = ((old_time - new_time) / old_time * 100) if old_time > 0 else 0

        old_intervention = simple["user_intervention_rate"]
        new_intervention = hybrid["user_intervention_rate"]
        intervention_reduction = (old_intervention - new_intervention) * 100

        simple_t1 = simple["tier1_hits"]
        simple_total = simple["total_errors"]
        hybrid_t1 = hybrid["tier1_hits"]
        hybrid_total = hybrid["total_errors"]
        t1_improvement = (hybrid_t1 - simple_t1) / simple_total * 100

        simple_t2 = simple["tier2_auto"] + simple.get("tier2_confirmed", 0)
        hybrid_t2_auto = hybrid["tier2_auto"]
        hybrid_t2_conf = hybrid.get("tier2_confirmed", 0)

        content = f"""# Performance Comparison Report

**Generated**: {datetime.now().isoformat()}

## Summary

| Metric | OLD (Simple) | NEW (Hybrid) | Improvement |
|--------|-------------|-------------|-------------|
| Automation Rate | {simple['automation_rate']:.1%} | {hybrid['automation_rate']:.1%} | +{auto_improvement:.1f}% |
| Resolution Time | {old_time:.1f}ms | {new_time:.1f}ms | -{time_reduction:.1f}% |
| User Intervention | {old_intervention:.1%} | {new_intervention:.1%} | -{intervention_reduction:.1f}%p |
| Tier 1 Hit Rate | {simple_t1/simple_total:.1%} | {hybrid_t1/hybrid_total:.1%} | +{t1_improvement:.1f}%p |

## Tier Distribution

### OLD System
- Tier 1: {simple_t1} ({simple_t1/simple_total*100:.1f}%)
- Tier 2: {simple_t2} ({simple_t2/simple_total*100:.1f}%)
- Tier 3: {simple['tier3_hits']} ({simple['tier3_hits']/simple_total*100:.1f}%)

### NEW System
- Tier 1: {hybrid_t1} ({hybrid_t1/hybrid_total*100:.1f}%)
- Tier 2 Auto: {hybrid_t2_auto} ({hybrid_t2_auto/hybrid_total*100:.1f}%)
- Tier 2 Confirmed: {hybrid_t2_conf} ({hybrid_t2_conf/hybrid_total*100:.1f}%)
- Tier 3: {hybrid['tier3_hits']} ({hybrid['tier3_hits']/hybrid_total*100:.1f}%)

## Key Improvements

1. **Automation**: +{auto_improvement:.1f}% improvement in automation rate
2. **Speed**: -{time_reduction:.1f}% reduction in resolution time
3. **User Time**: User intervention reduced from {old_intervention:.1%} to {new_intervention:.1%}

## Accuracy

- OLD System: N/A (no auto-apply)
- NEW System: {hybrid.get('accuracy', 'Not measured')}

## Conclusion

{(
    'The Hybrid Confidence-Based system shows significant improvement over the baseline.'
    if auto_improvement > 200
    else 'The Hybrid system shows moderate improvement.'
    if auto_improvement > 100
    else 'The Hybrid system shows modest improvement.'
)}

**Recommendation**: {(
    'Continue with current configuration'
    if hybrid['automation_rate'] > 0.6
    else 'Review configuration'
)}
"""

        with open(report_path, "w", encoding="utf-8") as f:
            f.write(content)

        print(f"\n[SAVED] Comparison report: {report_path}")


def main():
    comparison = PerformanceComparison()
    comparison.compare()

    print("\n" + "=" * 80)
    print("COMPARISON COMPLETE")
    print("=" * 80)
    print("\nNext steps:")
    print("1. Review RUNS/reports/comparison_*.md for detailed report")
    print("2. Check RUNS/benchmark/*.json for raw data")
    print("3. If performance is good, consider progressive enhancement:")
    print("   - Week 2: Lower threshold to 92%")
    print("   - Week 3: Lower threshold to 90%")
    print("4. Monitor circuit breaker status")

    return 0


if __name__ == "__main__":
    sys.exit(main())
